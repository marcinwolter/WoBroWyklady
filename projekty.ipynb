{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJEKTY\n",
    "## do przedmiotu Podstawy sieci neuronowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Zasady:**\n",
    "\n",
    "- Projekty są w dwóch grupach o różnej skali trudności: na maksymalną ocenę 5 i 4.5.\n",
    "\n",
    "- Wykonanie jednoosobowe.\n",
    "\n",
    "- Proszę wybrać projekt najpóźniej do 5.05. Jeśli będzie więcej niż jedna osoba na którąś z obecnych propozycji, to \"rozszczepię projekty\" na rozsądną liczbę wariantów.\n",
    "\n",
    "- Projekty można omawiać ze mną na ćwiczeniach w środy począwszy od 29.04 na ćwiczeniach w godz. 13:45-16:45 (są to dodatkowe ćwiczenia, oprócz tych w godz. 11:45-13:15).\n",
    "\n",
    "- Projekty maja być przygotowane jako jupyter python notebook. Mają zawierać autora, obfite komentarze i wyjaśnienia jako komórki \"markdown\".\n",
    "\n",
    "- Projekty będą przez autorów referowane na ćwiczeniach wg harmonogramu, który ustalimy później."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekty na maksymalną ocenę 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model pamięci heteroasocjatywnej.** Zastosuj poznany na wykładzie model pamięci heteroasocjatywnej do skojarzenia symboli a-A, b-B, c-C, d-D, e-E, f-F, g-G. Sprawdź, jak zaszumienie wpływa na przekłamywanie skojarzeń.\n",
    "\n",
    "   - Więcej pikseli, zależność działania od liczby pikseli.\n",
    "   - Wiecej symboli.\n",
    "\n",
    "2. **Model pamięci heteroasocjatywnej.** Zastosuj poznany na wykładzie model pamięci autoasocjatywnej do ropoznawania cyfr 0, 1, ..., 9. Sprawdź, jak zaszumienie wpływa na przekłamywanie rozpoznawania poszczególnych symboli.\n",
    "\n",
    "   - Więcej pikseli, zależność działania od liczby pikseli.\n",
    "   - Wiecej symboli.\n",
    "\n",
    "3. **Klasyfikacja obszarów z pomocą propagacji wstecznej.** Stosując poznany na wykładzie i w zadaniach algorytm propagacji wstecznej z sigmoidem, skonstruuj klasyfikator przynależności punktów do danego obszaru na płaszczyźnie. Warianty:\n",
    "\n",
    "   - Gwiazda - suma dwóch trójkątów.\n",
    "   - Ramka (kwadrat z \"wydrążonym\" w środku mniejszym kwadratem).\n",
    "   - Jakiś inny ulubiony obszar, którego nie było na wykładzie ani w zadaniach.\n",
    "\n",
    "4. **Uczenie nienadzorowane.** Stosując poznany algorytm uczenia nienadzorowanego ze strategią \"zwycięzca bierze wszystko\", skonstruuj klasyfikator punktów skupiajacych się w kilku klastrach na płaszczyźnie.\n",
    "\n",
    "   - Więcej klastrów (6-10).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekty na maksymalną ocenę 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Ulepszenie metody najstromszego spadku.** \n",
    "\n",
    "Stosowana w wykładzie metoda najstromszego spadku znajdowania minimum funkcji wielu zmiennych zależy od lokalnego gradientu minimalizowanej funkcji. Są znacznie lepsze podejścia, dające lepszą zbieżność do (lokalnego) minimum. Jednym z nich jest przepis [Barzilai-Borwein]\n",
    "\n",
    "$x_{n + 1} = x_n − \\gamma_n \\nabla F (x_n )$,\n",
    "\n",
    "gdzie $n$ numeruje krok a $x_i$ są wektorami w $D$-wymiarowej przestrzeni.\n",
    "\n",
    "$\\gamma _{n}=\\frac{\\left|\\left(x_n-x_{n-1}\\right) \\cdot \n",
    "\\left[\\nabla F(x_n)-\\nabla F(x_{n-1})\\right]\\right|}\n",
    "{\\left\\|\\nabla F(x_{n})-\\nabla F(x_{n-1})\\right\\|^{2}}$.\n",
    "\n",
    "Szybkość uczenia zależy od zachowania w dwóch (obecnym i poprzednim) punktach.\n",
    "\n",
    "- Zaprogamuj metodę do znajdowania minimum (prostych) funkcji dwóch zmiennych i przetestuj ją, robiąc wykresy podobne do tych na wykładzie.\n",
    "\n",
    "- Zaimplementuj metodę w algorytmie propagacji wstecznej.\n",
    "\n",
    "\n",
    "2. **Skalowanie wag w propagacji wstecznej.**\n",
    "\n",
    "Przypadłością stosowania sigmoidu w algorytmie propagacji wstecznej jest bardzo wolne uaktualnianie wag odległych od warstwy outputowej (im bliżej początku sieci, tym wolniej przebiega). Pewnym remedium jest tutaj przeskalowywanie wag, gdzie szybkość uczenia w warstwach licząc od tyłu jest zwiększana kolejno o pewien rosnący czynnik. Pamietamy, że kolejne pochodne wnoszą do szybkości uaktualniania czynniki postaci $\\sigma'(s)=\\sigma(s)[1-\\sigma(s)]=y(1-y)$, gdzie $y$ jest z przedziału $(0,1)$. Wartość $y(1-y$ nie może przekraczać 1/4, zatem w kolejnych warstwach (licząc od tyłu) iloczyn \n",
    "\n",
    "$[y(1-y]^n \\le 1/4^n$.\n",
    "\n",
    "Aby zapobiec temu maleniu, można szybkość uczenia mnożyć przez $4^n$:\n",
    "\n",
    "$4, 16, 64, 256, ...$.\n",
    "\n",
    "Inny heurystyczny argument [Rigler, Irvine, Vogl, 1989] sugeruje jeszcze szybciej rosnące czynniki postaci $6^n$:\n",
    "\n",
    "$6, 36, 216, 1296, ...$.\n",
    "\n",
    "\n",
    "- Wprowadź powyższe recepty do kodu dla propagacji wstecznej.\n",
    "\n",
    "- Sprawdź, czy polepszają działanie algorytmu dla stosowanych głębszych sieci, np. klasyfikator punktów w kole itp.\n",
    "\n",
    "- Pomiar czasu (pakiet time).\n",
    "\n",
    "\n",
    "3. **Uczenie nienadzorowane dla wiekszej liczby wymiarów.**\n",
    "\n",
    "Zaadaptuj przykład z wykładu klasyfikujący klastry na płaszczyżnie do przypadku trzech lub wiekszej liczby wymiarów. \n",
    "\n",
    "- Generowanie próbki z klastrami.\n",
    "\n",
    "- Puszczenie algorytmu np. z dynamicznym tworzeniem klastrów.\n",
    "\n",
    "- Dla trzech wymiarów zrób stosowne wykresy. Mogą to byc kolory RGB.\n",
    "\n",
    "- Dla większej liczby wymiarów wykreśl na płaszczyźnie rzutowania na wybrane 2 wymiary.\n",
    "\n",
    "\n",
    "4. **Optymalizacja sieci Kohonena.** \n",
    "\n",
    "Sprawdź zależność działania sieci Kohonena odwzorowującej kwadrat w linię (zob. wykład) od liczby neuronów $N$. Jak należy dobrać parametry $\\delta$ i $\\varepsilon$ w funkcji $N$, aby otrzymać rozwiązanie (konfiguracja bez \"przecięć\") w optymalnej liczbie iteracji (czasie)? \n",
    "Zbadaj systematycznie zależność od $\\delta$ przy ustalonym $\\varepsilon$, robiąc stosowne wykresy. \n",
    "\n",
    "\n",
    "5. **Sieć Kohonena odwzorowujaca sześcian w linię.**\n",
    "\n",
    "Modyfikacja przykładu z wykładu na przypadek danych o trzech wymiarach. Dobierz parametry $\\delta$ i $\\varepsilon$ w funkcji liczby neuronów $N$. Zrób wykres trójwymiarowy.\n",
    "\n",
    "\n",
    "6. **Sieć Kohonena odwzorowująca pewien dwuwymiarowy obszar na dwuwymiarową \"kratę\" neuronów.**\n",
    "\n",
    "Jest to ważny problem mapowania topologicznego. Wygeneruj punkty danych z jakiegoś obszaru (np. koło) i odwzoruj je na dwuwymiarową kratę neuronów. Warianty:\n",
    "\n",
    "- Koło, elipsa.\n",
    "\n",
    "- Trapez.\n",
    "\n",
    "- Dwa lub trzy oddzielone kwadraty (b. ciekawe!)\n",
    "\n",
    "\n",
    "7. **Amimacje do poznanych przykładów.**\n",
    "\n",
    "Czysto programistyczny projekt, mający za cel zrobienie animacji w pythonie obrazujących:\n",
    "\n",
    "- uaktualnianie wag podczas uczenia,\n",
    "\n",
    "- powstawanie właściwych obszarów w klasyfikatorach punktów na płaszczyźnie,\n",
    "\n",
    "- zmianę położenia punktów reprezentatywnych podczas uczenia nienadzorowanego \n",
    "\n",
    "- itp.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
